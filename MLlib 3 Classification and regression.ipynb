{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Запуск Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "spark_home='/opt/spark-1.5.1-bin-hadoop2.6/'\n",
    "# addin pyspark to current path\n",
    "sys.path.append( spark_home+'/python' )\n",
    "sys.path.append( spark_home+'/python/lib/py4j-0.8.2.1-src.zip' )\n",
    "\n",
    "# Установка локальных переменных\n",
    "os.environ[\"SPARK_HOME\"] = spark_home\n",
    "os.environ[\"HADOOP_HOME\"] = '/opt/cloudera/parcels/CDH/lib/hadoop'\n",
    "os.environ[\"HADOOP_YARN_HOME\"] = '/opt/cloudera/parcels/CDH/lib/hadoop-yarn'\n",
    "os.environ[\"YARN_CONF_DIR\"] = '/etc/hadoop/conf.cloudera.yarn'\n",
    "os.environ[\"SPARK_CLASSPATH\"] = '/etc/hive/conf.cloudera.hive1'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = '--master local[4] pyspark-shell'\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import HiveContext\n",
    "conf = SparkConf ().set( 'spark.app.name', 'test');\n",
    "sc = SparkContext (conf= conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка датасета, с которым мы проводим эсперименты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# загружаем датасет\n",
    "from pyspark.mllib.util import MLUtils\n",
    "examples = MLUtils.loadLibSVMFile(sc, \"/testdata/mllib_demo/libsvm_dataset\")\n",
    "\n",
    "\n",
    "# делим выборку на обучающую и тестовую\n",
    "(train, test) = examples.randomSplit ([0.7, 0.3], seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <br><br><br><br>Линейные классификаторы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/linear_classifier.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение лин. классификатора - задача выпуклой минимизации функционала от вектора весов $w$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(w) = \\lambda R(w) + \\frac{1}{n} \\sum_{i=1}^n L(w;x_i,y_i) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "где $x_i \\in  \\mathbb{R}$ - обучающие примеры,<br><br>\n",
    "$y \\in \\{-1,+1\\}$ - соответствующие метки<br><br>\n",
    "$L(w;x,y)$ - функция потерь<br><br>\n",
    "$R(w)$ - функция регуляризации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**Функции потерь** имеют вид:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Для SVM: $L(w;x,y) = \\max\\{0,1-yw^Tx\\}, y \\in \\{-1, +1\\}$<br><br>\n",
    "- Для логистической регрессии: $L(w;x,y) = log(1+\\exp(-yw^Tx)), y \\in \\{-1,+1\\}$<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Функции регуляризации** могут быть:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L1 - $||w||_2$ - дает более разреженные и интерпретируемые результаты\n",
    "- L2 - $\\frac{1}{2}||w||_2^2$ - более простая"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все эти задачи решаются в MLlib метдом стохастического градиентного спуска SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SVM: cl.SVMWithSGD\n",
    "- Логистическая регрессия: cl.LogisticRegressionWithSGD\n",
    "\n",
    "classifier.train(trainDataset) - обучает модель\n",
    "Параметры:\n",
    "- **data** – обучающая выборка RDD of LabeledPoint.\n",
    "- **iterations** – число итераций (по умолчанию: 100).\n",
    "- **step** – шаг SGD (по умолчанию: 1.0).\n",
    "- **miniBatchFraction** – доля данных которая используется на каждой SGD итерации\n",
    "- **initialWeights** – начальные веса (по умолчанию None)\n",
    "- **regParam** – параметр регуляризации (по умолчанию: 0.01)\n",
    "- **regType** – Тип регуляризатора (по умолчанию \"l2\")\n",
    " - **\"l1\"** для l1 регуляризации\n",
    " - **\"l2\"** для l2 регуляризации\n",
    " - **None** для отсутствия регуляризации\n",
    "- **intercept** – Булевый параметр, показывающий, применять ли дополнительное представление обучающих данных (то есть включены ли смещенные фичи или нет) ???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyspark.mllib.classification as cl\n",
    "\n",
    "model = cl.LogisticRegressionWithSGD.train (train, \n",
    "    iterations=500, \n",
    "    regType=\"l1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классифицируем результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plotROC (real_labels, predicted_labels):\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    fpr, tpr, _ = roc_curve(real_labels, predicted_labels)\n",
    "    roc_auc = auc (fpr, tpr)\n",
    "\n",
    "    %matplotlib inline\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.clf()\n",
    "    plt.plot(fpr,tpr)\n",
    "    print \"ROC AUC =\", roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы модель выдавала непрерывный результат, надо удалить у нее пороговое значение model.clearThreshold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.clearThreshold()\n",
    "plotROC (test.map (lambda x: x.label).collect(), \n",
    "         test.map (lambda x:model.predict(x.features)).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <br><br><br><br>Линейная регрессия "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы решает ту же задачу\n",
    "$$f(w) = \\lambda R(w) + \\frac{1}{n} \\sum_{i=1}^n L(w;x_i,y_i) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но функциноал потерь имеет вид <br> $L(w;x,y) = \\frac{1}{2} (w^Tx-y)^2, y \\in \\mathbb{R}$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разные типы регуляризации задают разные методы:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Если $R(w)=0$ то это **линейная регрессия** LinearRegressionWithSGD\n",
    "- Если $R(w)=||w||_2$ - **лассо** LassoWithSGD\n",
    "- Если $R(w)=\\frac{1}{2}||w||_2^2$ - **регрессия Риджа** RidgeRegressionWithSGD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметры обучения модели такие же как и у линейных классификаторов:<br>\n",
    "regression.train параметры:<br>\n",
    "- **data** – обучающая выборка RDD of LabeledPoint.\n",
    "- **iterations** – число итераций (по умолчанию: 100).\n",
    "- **step** – шаг SGD (по умолчанию: 1.0).\n",
    "- **miniBatchFraction** – доля данных которая используется на каждой SGD итерации\n",
    "- **initialWeights** – начальные веса (по умолчанию None)\n",
    "- **regParam** – параметр регуляризации (по умолчанию: 0.01)\n",
    "- **regType** – Тип регуляризатора (по умолчанию None)\n",
    " - **\"l1\"** для l1 регуляризации (лассо)\n",
    " - **\"l2\"** для l2 регуляризации (ридж)\n",
    " - **None** для отсутствия регуляризации\n",
    "- **intercept** – Булевый параметр, показывающий, применять ли дополнительное представление обучающих данных (то есть включены ли смещенные фичи или нет) ???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyspark.mllib.regression as reg\n",
    "model = reg.RidgeRegressionWithSGD.train (train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotROC (test.map(lambda x:x.label).collect(),\n",
    "         test.map(lambda x: model.predict(x.features)).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <br><br><br><br>Наивный байес"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/Bayes.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм обучения наивного байеса NaiveBayes.train принимает на вход всего 2 параметра\n",
    "- **data** обучающую выборку\n",
    "- **lab** параметр сглаживания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?NaiveBayes.train(train, 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import NaiveBayes\n",
    "\n",
    "model = NaiveBayes.train(train, 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.predict(test.take(10)[5].features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotROC (test.map (lambda x: x.label).collect(),\n",
    "         test.map (lambda x: model.predict (x.features)).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <br><br><br><br>Решающие деревья"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/decision_tree_example.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При обучении дерева максимизируеся impuruty после разбиения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gini:  $\\sum_{i=1}^{C} f_i (1-f_i)$<br><br>\n",
    "- entropy $\\sum_{i=1}^{C} -f_i log(f_i)$<br><br>\n",
    "- vairance $\\frac{1}{N} \\sum_{i=1}^{N} (y_i-\\mu)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение алгоритматами\n",
    "DecisionTree.trainClassifier\n",
    "и\n",
    "DecisionTree.trainRegressor\n",
    "\n",
    "с параметрами \n",
    "- **data** – обучающая выборка, RDD\n",
    "- **numClasses** – число классов (только для классификатора)\n",
    "- **categoricalFeaturesInfo** – указание, какие у категориальных фич есть категории\n",
    "- **impurity** – Возможные значения\n",
    " - **“entropy”** (для классификатора)\n",
    " - **“gini”** (для классификатора)\n",
    " - **“variance”** (для регрессии)\n",
    "- **maxDepth** – макс глубина дерева. Глубина 0 - 1 лист, Глубина 1 - 1 внутренняя нода и 2 листа\n",
    "- **maxBins** – число бакетов, по которому ищутся сплиты по каждому из  узлов.\n",
    "- **minInstancesPerNode** – мин. число инстансов, которые требуются дочерним нодам для созданя родительсткого сплита\n",
    "- **minInfoGain** – минимальный уровень информации, для создания сплита\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import DecisionTree\n",
    "\n",
    "model = DecisionTree.trainClassifier(train, \n",
    "                                     numClasses=2, \n",
    "                                     categoricalFeaturesInfo={},\n",
    "                                     impurity='entropy',\n",
    "                                     maxDepth=3, \n",
    "                                     maxBins=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotROC (test.map (lambda x: x.label).collect(),\n",
    "         model.predict(test.map (lambda x: x.features)).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print model.toDebugString()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <br><br><br><br>Случайный лес"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/random_forest.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random forest использует деревья, описанные выше.\n",
    "Так же как и у деревьев, у случайного леса есть методы\n",
    "- **trainClassifier** для обучения классификатора\n",
    "- **trainRegression** для обучения функции регрессии\n",
    "\n",
    "Параметры:\n",
    "\n",
    "- **data** – обучающая выборка, RDD LabeledPoint. Классов может быть несколько\n",
    "- **numClasses** – число классов для классификации\n",
    "- **categoricalFeaturesInfo** – информация по категорийным фичам. Запись (n -> k) показывает, что фича n - категориальная и имеет k категорий, начинащихся с 0: $\\{0, 1, ..., k-1\\}$.\n",
    "- **numTrees** – число деревьев в лесу\n",
    "- **featureSubsetStrategy** - число фич, которые рассматриваются для сплитов на каждой из нод. Возможные варианты:\n",
    " -**“auto”** (по умолчаниюб если дерево 1- выбираются все фичи, иначе берется корень из числа фич)\n",
    " -**“all”**\n",
    " -**“sqrt”**\n",
    " -**“log2”**\n",
    " -**“onethird”**\n",
    "- **impurity** – критерий вычисления information gain. Значения:\n",
    " - “gini” (для классификации, рекомендуются)\n",
    " - “entropy” (для классификации)\n",
    " - \"variance\" (для задач регрессии)\n",
    "- **maxDepth** – максимальная глубина дерева\n",
    "- **maxBins** – максимальное число корзин, используемое для разделения фич (по умолчанию 100)\n",
    "- **seed** – Случайный seed для бутстрепинга и выбора подмножеств фич"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "?RandomForest.trainClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest\n",
    "model = RandomForest.trainClassifier(train, \n",
    "                                     numClasses=2, \n",
    "                                     categoricalFeaturesInfo={},\n",
    "                                     numTrees=300, \n",
    "                                     featureSubsetStrategy=\"auto\",\n",
    "                                     impurity='gini', \n",
    "                                     maxDepth=20, \n",
    "                                     maxBins=32,\n",
    "                                     seed=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotROC (test.map(lambda x: x.label).collect(),\n",
    "         model.predict (test.map(lambda x:x.features)).collect())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
