{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://spark.apache.org/images/spark-logo.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Что такое Apache spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Библиотека для распределенных вычислений\n",
    "- отказоустойчивая\n",
    "- итеративная\n",
    "- интерактивная\n",
    "- интегрирована с hadoop\n",
    "- написана на scala \n",
    "- очень популярная"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Как запускать pyspark notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "spark_home='/opt/spark-1.5.1-bin-hadoop2.6/'\n",
    "# addin pyspark to current path\n",
    "sys.path.append( spark_home+'/python' )\n",
    "sys.path.append( spark_home+'/python/lib/py4j-0.8.2.1-src.zip' )\n",
    "\n",
    "# Установка локальных переменных\n",
    "os.environ[\"SPARK_HOME\"] = spark_home\n",
    "os.environ[\"HADOOP_HOME\"] = '/opt/cloudera/parcels/CDH/lib/hadoop'\n",
    "os.environ[\"HADOOP_YARN_HOME\"] = '/opt/cloudera/parcels/CDH/lib/hadoop-yarn'\n",
    "os.environ[\"YARN_CONF_DIR\"] = '/etc/hadoop/conf.cloudera.yarn'\n",
    "os.environ[\"SPARK_CLASSPATH\"] = '/etc/hive/conf.cloudera.hive1'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = '--master local[4] pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import HiveContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf = SparkConf ().set( 'spark.app.name', 'test');\n",
    "sc = SparkContext (conf= conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обработка данных на спарке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как выглядит программа на спарке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Получаем данные\n",
    "data = [1, 2, 3, 4, 5]\n",
    "inputRDD = sc.parallelize(data)\n",
    "\n",
    "# Преобразовываем данные\n",
    "filteredRDD = inputRDD.filter(lambda x: x%2==0)\n",
    "\n",
    "# Сохраняем / выдаем данные\n",
    "print \"число элементов в исходном датасете\", inputRDD.count ()\n",
    "print \"число элементов в конечном датасете\", filteredRDD.count()\n",
    "print \"конечный датасет:\", filteredRDD.collect ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"тип спарковского датасета:\", type (filteredRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RDD** (Resilient Distributed Dataset) - распределенные датасеты, с которыми работает spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='http://1.bp.blogspot.com/-GhyDINiSlRY/Uu9BH844SBI/AAAAAAAABDM/J12X3mHPw1E/s1600/P2.png' width='400'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как заводить данные в spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружать датасеты из локальной программы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "inputRDD = sc.parallelize(data)\n",
    "print \"число элементов\", inputRDD.count ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружать датасетсы из hdfs / s3. TODO.  Разобраться с этим. возможно поднять hadoop кластер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputRDD = sc.textFile(\"/testdata/splits_test\")\n",
    "print \"число элементов\", inputRDD.count ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кеширование данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD датасеты можно кешировать и хранить в оперативной памяти. \n",
    "Это позволяет\n",
    "- делать итеративные операции над данными (напр. в машинном обучении)\n",
    "- интерактивно исследовать данные\n",
    "\n",
    "кеширование выполняется при помощи операции **cache** с датасетом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputRDD.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Простые преобразования"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Трансформации преобразовывают одни RDD в другие. Такие преобразования являются промежуточным, а не конечным результатом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr><td>**Преобразование**</td> <td>**Описание**</td></tr>\n",
    "\n",
    "<tr><td>**map**(func)</td> <td>Вовзращает новый RDD полученный из применения функции funk к каждому элементу исходного датасета</td></tr>\n",
    "\n",
    "<tr><td>**filter**(func)</td> <td>Возвращает датасет из элементов, для которых функция *funk* вернула *true*</td></tr>\n",
    "\n",
    "<tr><td>**flatMap**(func)</td> <td> Аналогично функции **map** но каждый входной элемент может быть преобразован в 0 и более выходных элементов. (Поэтому функция должна возвращать список вместо одиночного объекта).</td></tr>\n",
    "\n",
    "<tr><td>**distinct**([numTasks]))</td> <td>Возвращает новый датасет, который содержит разные элементы исходного датасета.</td></tr>\n",
    "\n",
    "<tr><td>**sample**(withReplacement, fraction, seed)</td> <td>Выдает долю *fraction* данных с возвращением или без, используя заданный seed генератора случайных чисел</td></tr>\n",
    "\n",
    "<tr><td>**union**(otherDataset)</td> <td>Возвращает новый датасет, который содерживт объединения элементов исходного датасета и датасета - аргумента </td></tr>\n",
    "\n",
    "<tr><td>**intersection**(otherDataset)</td> <td>Вовзращает новый RDD, который содержит пересечение элементов исходного датасета с датасетом - аргументом</td></tr>\n",
    "\n",
    "<tr><td>**cartesian**(otherDataset)</td> <td>Декартово произведение. При применении к датасетам типов T и U, возвращает датасет пар (T,U) (все возможные пары элементов)</td></tr>\n",
    "\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для примера будет разбирать лог действий пользователей в некоторой системе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Будем разбирать вот этот датасет\n",
    "for l in inputRDD.top(5): print l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**map** позволяет разделить строки на поля и выбрать только интересные нам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Функция map позволяет разделить строки на поля и выбрать только интересные нам:\n",
    "splittedRDD = inputRDD.map (lambda x: x.split(\";\")[:])\n",
    "\n",
    "# Выводим на экран при помощи pandas data frame\n",
    "import pandas\n",
    "pandas.DataFrame(splittedRDD.top(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**filter** позволяет исключать из выборки строки, не попадающие под условие. \n",
    "\n",
    "Отфильтровываем заголовки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "filteredRDD = splittedRDD.filter (\n",
    "lambda x: len (re.findall (\"^\\d{2}\\.\\d{2}.\\d{4}.*\", x[0])) != 0 ) \n",
    "pandas.DataFrame(filteredRDD.top(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**flatMap** позволяет разделить одну строку записей на несколько строк. \n",
    "\n",
    "Например если нам нужно выделить каждый параметр операции в отдельную строку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parsedParamsRDD = filteredRDD.flatMap (\n",
    "lambda x: [[x[0],x[1],x[2],x[3],param] \n",
    "           for param in x[4].split(',')] )\n",
    "pandas.DataFrame(parsedParamsRDD.top(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sample** - простое решение задачи семплирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print splittedRDD.count()\n",
    "sampledRDD = splittedRDD.sample (False, 0.1, 3)\n",
    "print sampledRDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**distinct** - способ удаления дублей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users = filteredRDD.map(lambda x: x[1])\n",
    "distinct_users = users.distinct()\n",
    "print \"rows\", users.count()\n",
    "print \"distinct users\", distinct_users.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**union** и **intersection** - объединение / пересечение двух датасетов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uniqueRDD = inputRDD.distinct()\n",
    "sample1 = uniqueRDD.sample(False, 0.1, 1)\n",
    "sample2 = uniqueRDD.sample(False, 0.1, 2)\n",
    "\n",
    "print \"input size:         \", uniqueRDD.count()\n",
    "print \"sample 1 size:      \", sample1.count()\n",
    "print \"sample 2 size:      \", sample2.count()\n",
    "print \"union size:         \", sample1.union(sample2).count ()\n",
    "print \"intresection size:  \", sample1.intersection(sample2).count ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Действия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Действия превращают RDD во что-то другое. Например сохраняют в файл или возвращают значение в блокнот"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr><td>**Action**</td> <td>**Meaning**</td></tr>\n",
    "<tr><td>**reduce**(func)</td> <td>Аггрегирует элементы датасета, используя функцию *func* (которая принимает 2 аргумента и возвращает 1 значение). Функция должна быть коммутативной и ассоциативной так чтобы она могла правильно паралельно вычисляться</td></tr>\n",
    "\n",
    "<tr><td>**collect**()</td> <td>\n",
    "Вовзращат все элементы датасета как массив программе - драйверу. Это обычно полезно после фильтрации или другой операции, которая возвращает достаточно маленькое подмножество данных</td></tr>\n",
    "\n",
    "<tr><td>**count**()</td> <td>Возвращает число элементов в датасете</td></tr>\n",
    "\n",
    "<tr><td>**first**()</td> <td>Возвращает 1й элемент множество (аналогично *take (1)*)</td></tr>\n",
    "\n",
    "<tr><td>**take**(n)</td> <td>\n",
    "Возвращает массив первых *n* элементов в датасете. Помните, что сейчас этот метод выполняется не паралельно, а вместо этого программа-драйвер обсчитывает все элементы</td></tr>\n",
    "\n",
    "<tr><td>**takeSample**(withReplacement, num, seed)</td> <td>\n",
    "Возвращет массив со случайным семплом из num элементов датасета, с или без возвращения, используя заданный seed генератора случайных чисел\n",
    "</td></tr>\n",
    "\n",
    "<tr><td>**takeOrdered**(n, [ordering])</td> <td>\n",
    "Возвращает первые *n* элементов RDD используя либо натуральный порядок или заданный компаратор</td></tr>\n",
    "\n",
    "<tr><td>**saveAsTextFile**(path)</td> <td>\n",
    "Пишет элементы датасета в текстовый файл (или набор текстовых файлов) в заданную дирекорию на логальной файловой системе, HDFS или любой другой файловой системе, поддерживаемой хадупом. Спарк будет вызывать *toString* для каждого элемента для преобразования его в строку теста в файле</td></tr>\n",
    "\n",
    "<tr><td>**foreach**(func)</td> <td>\n",
    "Выполняет функцию *funk* для каждого элемента в датасете. Это обычно делается для побочных действий, таких как обновления аккумуляторов или  взаимодействия с внешней системой храния</td></tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**reduce** позволяет посчитать какую-нибудь статистику по датасету.<br>\n",
    "Например найти минимальное и максимальное время лога"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "times = filteredRDD.map(lambda x:x[0])\n",
    "print times.reduce(min), \"-\", times.reduce(max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='http://1.bp.blogspot.com/-NhP-r7kTpIw/UBfkiMj_1lI/AAAAAAAAAJ0/CV1gFLfNLW0/s1600/Types%2Bof%2BNoSQL%2B-%2Bkey%2Bvalue%2Bstore.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа с датасетами пар ключ значение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для чего надо выделять ключ для записей в датасете?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- для группировки данных по ключу\n",
    "- для джойна датасетов по ключу\n",
    "- для сортировки\n",
    "\n",
    "В общем выделение ключа очень полезно!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как выделить ключ из датасета?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделить ключ из датасета, можно, превратив строки в списки из 2х и более значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputRDD = sc.textFile(\"/testdata/splits_test\")\n",
    "# splitted RDD уже имеет первичный ключ \"Субъект\", то естьтот, кто делал операцию\n",
    "from operator import itemgetter\n",
    "keyValueRDD = inputRDD.map (lambda x: itemgetter(1,2,3,4,0)(x.split(\";\")))\n",
    "import pandas\n",
    "pandas.DataFrame(keyValueRDD.top(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Что можно делать с key-value датасетом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr><td>**Преобразование**</td> <td>**Описание**</td></tr>\n",
    "\n",
    "<tr><td>**groupByKey**([numTasks])</td> <td>Если применить к датасету пар (K, V), вернет датасет пар (K, Iterable&lt;V&gt;). \n",
    "<br>**Note:** При группировке для рассчета аггрегатов (таких как сумма или среднее), рекоммендуется использовать reduceByKey или combineByKey для лучшей производительности\n",
    "\n",
    "<br>**Note:** По умолчанию уровень паралелизма на выходе зависит от числа партиций в родительском RDD. Можно передать опциональный аргумент numTasks для установки другого числа задач</td></tr>\n",
    "\n",
    "<tr><td>**reduceByKey**(func, [numTasks])</td> <td>\n",
    "При применении к датасету пар (k,v), возвращает новый датасет пар (K,V), где значения каждого ключа саггрегированны, через заданную reduce функцию. Как в groupByKey, число reduce задач задается необязательным вторым аргументом</td></tr>\n",
    "\n",
    "<tr><td>**sortByKey**([ascending], [numTasks])</td> <td>\n",
    "При применении к датасету пар (K,V), где K может быть отсортирован, возвращает датасет пар (K,V), отсортированных по ключу в порядке убывания или возрастания, в зависимости от первого аргумента</td></tr>\n",
    "\n",
    "<tr><td>**join**(otherDataset, [numTasks])</td> <td>\n",
    "При применении к датасетам (K,V) и (K,W), возвращает датасет пар (K,(V,W)), со всеми парами элементов для каждого из ключей. Внешние джойны также поддерживаются через leftOuterJoin и rightOuterJoin</td></tr>\n",
    "\n",
    "<tr><td>**cogroup**(otherDataset, [numTasks])</td> <td>\n",
    "При примененеии к датасетам типов (K,V) и (K,W), возвращает датасет кортежей (K, Iterable &lt;V&gt;, Iterable &lt;W&gt;). Эта операция также называется groupWith</td></tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr><td>**Действие**</td> <td>**Описание**</td></tr>\n",
    "<tr><td>**countByKey**()</td> <td>Only available on RDDs of type (K, V). Returns a hashmap of (K, Int) pairs with the count of each key.</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Примеры операций"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupByKey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выпишем все действия для каждого из пользователей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "groupedRDD = keyValueRDD.map(lambda x:(x[0],x[4])).groupByKey()\n",
    "pandas.DataFrame(groupedRDD.top(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduceByKey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдем время начала и конца работы пользователя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def findMinMax (minMax, newValue):\n",
    "    newMin = newValue[0] if newValue[0] < minMax[0] else minMax[0]\n",
    "    newMax = newValue[1] if newValue[1] > minMax[1] else minMax[1]\n",
    "    return (newMin, newMax)\n",
    "\n",
    "minMaxRDD = keyValueRDD.map(lambda x: (x[0],(x[4],x[4]))).reduceByKey(findMinMax)\n",
    "pandas.DataFrame(minMaxRDD.top(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sortByKey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сортируем датасет по разным ключам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted1RDD = keyValueRDD.map(lambda x: (x[1],x)).sortByKey ()\n",
    "pandas.DataFrame(sorted1RDD.top(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "sorted4RDD = keyValueRDD.map(lambda x: (random.randint(0,1000000),x)).sortByKey (ascending=True)\n",
    "pandas.DataFrame(sorted4RDD.top(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Старый добрый джойн как в sql. Давайте пересечь два семпла датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distinctRDD = keyValueRDD.map(lambda x:tuple(x))\\\n",
    "                         .distinct()\\\n",
    "                         .map(lambda x: (x[0]+\" - \" + x[4], x))\n",
    "        \n",
    "sampledRDD1 = distinctRDD.sample (False, 0.1, 1)\n",
    "sampledRDD2 = distinctRDD.sample (False, 0.1, 2)\n",
    "\n",
    "joinedRDD = sampledRDD1.join (sampledRDD2)\n",
    "\n",
    "# Сколько строк оказалось в обоих датасетах?\n",
    "print \"Размер перечесения двух выборок\", joinedRDD.count()\n",
    "\n",
    "# Сколько разных записей оказалось имеют один ключ\n",
    "print \"Число случаев, когда ключ оказался уникальным\", \n",
    "print joinedRDD.filter(lambda x:x[1][0] == x[1][1]).count()\n",
    "\n",
    "pandas.DataFrame(joinedRDD.map(lambda x:(x[0],x[1][0],x[1][1])).top(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### countByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# countByKeyTable - это уже локальный словарик\n",
    "countByKeyTable = keyValueRDD.countByKey()\n",
    "pandas.DataFrame(countByKeyTable.items()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Литература"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='http://spark.apache.org/docs/latest/programming-guide.html'/>\n",
    "<img src='http://spark.apache.org/docs/latest/img/spark-logo-hd.png' align='left' width=70> <br>&nbsp;&nbsp;&nbsp;Spark programming guide\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border='0' color=\"#ff00ff\">\n",
    "<tr>\n",
    "<td>\n",
    "<a href=\"http://shop.oreilly.com/product/0636920028512.do\">\n",
    "<img src=\"http://akamaicovers.oreilly.com/images/0636920028512/cat.gif\" />\n",
    "**Holden Karau** Learning Spark\n",
    "</a>\n",
    "</td>\n",
    "<td>\n",
    "<a href=\"http://it-ebooks.info/book/5970/\">\n",
    "<img src=\"http://it-ebooks.info/images/ebooks/3/advanced_analytics_with_spark.jpg\" />\n",
    "**Sandy Ryza** Advanced Analytics with Spark\n",
    "</a>\n",
    "</td>\n",
    "<td>\n",
    "<a href=\"http://it-ebooks.info/book/6809/\">\n",
    "<img src=\"http://it-ebooks.info/images/ebooks/14/spark_for_python_developers.jpg\" />\n",
    "**Amit Nandi Spark** for Python Developers\n",
    "</a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
