{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Запуск Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "spark_home='/opt/spark-1.5.1-bin-hadoop2.6/'\n",
    "# addin pyspark to current path\n",
    "sys.path.append( spark_home+'/python' )\n",
    "sys.path.append( spark_home+'/python/lib/py4j-0.8.2.1-src.zip' )\n",
    "\n",
    "# Установка локальных переменных\n",
    "os.environ[\"SPARK_HOME\"] = spark_home\n",
    "os.environ[\"HADOOP_HOME\"] = '/opt/cloudera/parcels/CDH/lib/hadoop'\n",
    "os.environ[\"HADOOP_YARN_HOME\"] = '/opt/cloudera/parcels/CDH/lib/hadoop-yarn'\n",
    "os.environ[\"YARN_CONF_DIR\"] = '/etc/hadoop/conf.cloudera.yarn'\n",
    "os.environ[\"SPARK_CLASSPATH\"] = '/etc/hive/conf.cloudera.hive1'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = '--master local[4] pyspark-shell'\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import HiveContext\n",
    "conf = SparkConf ().set( 'spark.app.name', 'test');\n",
    "sc = SparkContext (conf= conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Фичи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/dense_vector.PNG'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обычный \"плотный\" вектор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "в MLlib можно задавать его\n",
    "- через список \n",
    "  [-2.91, 4.08, 0, 0, 0, 0, 2.48, 0, 0, 1.46]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = sc.parallelize([[-2.91, 4.08, 0, 0, 0, 0, 2.48, 0, 0, 1.46],\n",
    "  [-2.91, 4.08, 0, 0, 0, 0, 2.48, 0, 0, 1.46]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- через numpy.array (что более эффективно) np.array([-2.91, 4.08, 0, 0, 0, 0, 2.48, 0, 0, 1.46])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.array([-2.91, 4.08, 0, 0, 0, 0, 2.48, 0, 0, 1.46])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- класс \"вектор\" Vectors.dense ([-2.91, 4.08, 0, 0, 0, 0, 2.48, 0, 0, 1.46])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "v = Vectors.dense ([-2.91, 4.08, 0, 0, 0, 0, 2.48, 0, 0, 1.46])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <br><br><br><br> Разреженный вектор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задавать разреженные вектора можно при помощи \n",
    "- SciPy csc_matrix с одной колонкой<br>\n",
    "Пример: sv2 = sps.csc_matrix((np.array([1.0, 10.0]), np.array([0, 1, 6, 9]), np.array([-2.91, 4.08, 2.48, 1.46])), shape = (10, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use a single-column SciPy csc_matrix as a sparse vector.\n",
    "import scipy.sparse as sps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MLibs SparseVector<br>\n",
    "Пример: Vectors.sparse(10, [0,1,6,9], [-2.91, 4.08, 2.48, 1.46])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "a = Vectors.sparse(10, [0,1,6,9], [-2.91, 4.08, 2.48, 1.46])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br>\n",
    "<h2>Размеченные точки</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/labeled_point.PNG'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс LabeledPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "LabeledPoint (1, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LabeledPoint.features** - любой из описанных выше векторов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LabeledPoint.label** - float<br>\n",
    "Может быть:\n",
    "- 0.0, 1.0 для бинарной классификации\n",
    "- 0.0, 1.0, 2.0, ... для мультиклассовой классификации\n",
    "- любое число для регрессии    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br>\n",
    "<h2>Загрузка данных</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно создать датасет LabeledPoints самостоятельно<br>\n",
    "- Файл с примерами <a href='http://5.9.102.237:8888/filebrowser/view//testdata/kaggle/train.csv'>/testdata/kaggle/train.csv</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = sc.textFile (\"/testdata/kaggle/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data\n",
    ".map(lambda x: [float(v) for v in x.split(\"\\t\")])\\\n",
    ".map (lambda x: LabeledPoint (x[0], x[1:])).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <br><br><br><br><br><br><br><br> Загрузка Lib SVM файлов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У lib svm есть свой формат хранения разреженных фич<br>\n",
    "Пример: <a href='http://5.9.102.237:8888/filebrowser/view//testdata/mllib_demo/libsvm_dataset/part-00000'>/testdata/mllib_demo/libsvm_dataset</a><br>\n",
    "Команда MLUtils.loadLibSVMFile(sc,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! hadoop fs -cat /testdata/mllib_demo/libsvm_dataset/part-00000 | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "svm_result = MLUtils.loadLibSVMFile(sc, \"/testdata/mllib_demo/libsvm_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svm_result.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Остановка Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
