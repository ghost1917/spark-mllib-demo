{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "spark_home='/opt/spark-1.5.1-bin-hadoop2.6/'\n",
    "# addin pyspark to current path\n",
    "sys.path.append( spark_home+'/python' )\n",
    "sys.path.append( spark_home+'/python/lib/py4j-0.8.2.1-src.zip' )\n",
    "\n",
    "# Установка локальных переменных\n",
    "os.environ[\"SPARK_HOME\"] = spark_home\n",
    "os.environ[\"HADOOP_HOME\"] = '/opt/cloudera/parcels/CDH/lib/hadoop'\n",
    "os.environ[\"HADOOP_YARN_HOME\"] = '/opt/cloudera/parcels/CDH/lib/hadoop-yarn'\n",
    "os.environ[\"YARN_CONF_DIR\"] = '/etc/hadoop/conf.cloudera.yarn'\n",
    "os.environ[\"SPARK_CLASSPATH\"] = '/etc/hive/conf.cloudera.hive1'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = '--master local[4] pyspark-shell'\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import HiveContext\n",
    "conf = SparkConf ().set( 'spark.app.name', 'test');\n",
    "sc = SparkContext (conf= conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наши датасеты для экспериментов\n",
    "- examples - массив размеченных данных\n",
    "- features - фичи массива examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "examples = MLUtils.loadLibSVMFile(sc, \"/testdata/mllib_demo/libsvm_dataset/\")\n",
    "features = examples.map(lambda x:x.features)\n",
    "labels = examples.map(lambda x:x.label)\n",
    "print features.first().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <br><br><br><br> Статистика фич"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисляется методом stat.colStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "summary = Statistics.colStats(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что есть полезного в результатах?\n",
    "- среднее значение фичи summary.mean()\n",
    "(всегод фич 119, рекомендуется взять первые 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summary.numNonzeros()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- дисперсия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summary.variance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- минимум, максимум, число ненулевых элементов и т.д."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summary.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <br><br><br><br>Корреляция"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Корреляция. Можно посчитать корреляцию как между разными элементами одного датасета,\n",
    "так и двумя разными датасетами\n",
    "Statistics.corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для примера найдем фичи, коррелирующие с целевой функцией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features.map(lambda x: x[11]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correlation = [(i, Statistics.corr(features.map(lambda x: x[i]), labels)) for i in xrange (features.first().size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отсортировать по корреляции и взять топ 5 ответов (sorted ( ... reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted(correlation, reverse=True, key=lambda x:abs(x[1]))[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо корреляции двух векторов, corr позволяет вычислить корреляции всех столбцов в матрице между собой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Statistics.corr (features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <br><br><br><br><br>Семплирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не входит в MLlib, но об этом стоит упомянуть при построении алгоритмов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Случайные семплы**<br>\n",
    "(для выделения обучающей и тестовой выборок)<br>\n",
    "RDD.randomSplit([weight1, weight2,...], seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(train, test) = examples.randomSplit([0.7, 0.3], 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Стратифицированные семплы**<br>\n",
    "Когда нужно отобрать заданную долю для каждого из значений.\n",
    "Например в имеющемся датасете перекос по ключам<br>\n",
    "fractions = {\"a\": 0.2, \"b\": 0.1}<br>\n",
    "RDD.sampleByKey(withReplacement, fractions, seed)<br>\n",
    "Работает только для датасетов вида ключ-значение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Выделяем целевую функцию в качестве ключа, считаем частоты\n",
    "examples.map (lambda x: (x.label, x)).countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "examples.map (lambda x: (x.label, x))\\\n",
    ".sampleByKey (False,\n",
    "              {0:0.33,  # 25% примеров с целевой функцией 0\n",
    "               1:1},    # 100% примеров с целевой функцией 1\n",
    "              42)\\\n",
    ".countByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <br><br><br><br><br><br><br><br><br>Генерация случайных данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть распределения:\n",
    "- **uniformRDD** (sc, size, numPartitions, seed)\n",
    "- **normalRDD** (sc, size, numPartitions, seed)\n",
    "- **poissonRDD** (sc, size, numPartitions, seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.random import RandomRDDs\n",
    "# Пример нормального распределения m=0, sigma=1\n",
    "RandomRDDs.normalRDD(sc, 1000, 10, 31).take(10)\n",
    "\n",
    "\n",
    "# Переделываем распределение в m=100, sigma=0.01\n",
    "RandomRDDs.uniformRDD(sc, 1000, 10, 31).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <br><br><br><br><br>TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть корпус документов $D$<br>\n",
    "$t$ - некоторый термин в нем<br>\n",
    "$d$ - некоторый документ в нем<br>\n",
    "<br>\n",
    "$TF(t,d)$ - сколько раз встретился термин $t$ в документе $d$<br>\n",
    "<br>\n",
    "$DF(t,D)$ - в скольки документах корпуса встечается термин $t$<br>\n",
    "<br>\n",
    "$IDF(t,D)=\\log \\frac{|D|+1}{DF(t,d)+1}$<br>\n",
    "<br>\n",
    "$TFIDF(t,d,D)=TF(t,d) \\cdot IDF(t,D)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В примере используется датасет из англоязычных абстрактов википедии<br>\n",
    "\"/testdata/mllib_demo/wiki_abstracts/abstrats1000.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF**<br>\n",
    "Частоты терминов считаются с использованием трюка с хешированием, чтобы не хранить связку термин-id<br>\n",
    "hashingTF.transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import HashingTF\n",
    "\n",
    "docs = sc.textFile (\n",
    "\"/testdata/mllib_demo/wiki_abstracts/abstrats1000.txt\")\n",
    "splitted_docs = docs.map (lambda x: x.split(\" \"))\n",
    "\n",
    "hashingTF = HashingTF()\n",
    "tf = hashingTF.transform(splitted_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF**<br>\n",
    "Считается за 2 прохода\n",
    "- сначала считается IDF<br> \n",
    " IDF(minDocFreq).fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import IDF\n",
    "\n",
    "idf = IDF(minDocFreq=2).fit(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- затем умножается на TF<br>\n",
    " idf.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf = idf.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <br><br><br><br><br> Пересчитаем TF-IDF из хешей в слова"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала надо построить словарь преобразования хеш - слово<br>\n",
    "id2word = documents.flatMap (lambda x:x)\\<br>\n",
    "         .distinct ()\\<br>\n",
    "         .map (lambda x: (hashingTF.indexOf(x),x))\\<br>\n",
    "         .collectAsMap ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перебираем tf-idf и вместо каждого индекса подставляем слово\n",
    "tfidf_words = tfidf.map (lambda doc:<br> \n",
    "                   zip(map (lambda x: (x,id2word [x]), doc.indices),<br>  doc.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id2word = splitted_docs.flatMap (lambda x:x)\\\n",
    ".distinct ()\\\n",
    ".map (lambda x: (hashingTF.indexOf(x),x))\\\n",
    ".collectAsMap ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf.map (lambda doc:\n",
    "zip(map (lambda x: (x,id2word [x]), doc.indices),\n",
    "doc.values)).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <br><br><br><br><br>Масштабирование фич"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приведение фич к одному масштабу помогает сходимости итерационных методов поиска оптимумов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"0\">\n",
    "<tr>\n",
    "<td>\n",
    "<img src='images/features_scaled_gd.png'>\n",
    "</td>\n",
    "<td>\n",
    "<img src='images/features_normal_gd.png'>\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td align=\"center\">\n",
    "до масштабирования\n",
    "</td>\n",
    "<td align=\"center\">\n",
    "после масштабирования</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! hadoop fs -ls /testdata/mllib_demo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Загружаем наш массив примеров\n",
    "from pyspark.mllib.util import MLUtils\n",
    "examples = MLUtils.loadLibSVMFile(sc, \"/testdata/mllib_demo/libsvm_dataset\")\n",
    "\n",
    "# Выделяем фичи фичи примера\n",
    "features = examples.map(lambda x:x.features)\n",
    "labels = examples.map(lambda x:x.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основной класс, который это делает - StandardScaler\n",
    "- в начале строим модель преобразований<br> StandardScaler().fit(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import StandardScaler\n",
    "\n",
    "scaler1 = StandardScaler(withMean=True, withStd=True).fit(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler1.transform (features.map(lambda x:x.toArray())).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Затем преобразовываем по ней фичи<br>\n",
    "model.transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Внимание!**<br>\n",
    "масштабирование с установкой среднего работает только с плотными векторами<br>\n",
    "StandardScaler(withMean=True, withStd=True).fit(dense_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <br><br><br><br><br><br><br>Нормализация семплов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При масштабировании мы работали с фичами. При нормализации - с семплами. \n",
    "Полезно для сравнения косинусного расстояния между двумя векторами\n",
    "<img src='images/normalization.png'>\n",
    "\n",
    "Запуск: Normalizer().transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import Normalizer\n",
    "Normalizer().transform(features).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>Можно нормализовывать фичи не по метрике $L^2$ а по $L^\\infty$\n",
    "\n",
    "Запуск: Normalizer(p=float(\"inf\")).transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Normalizer(p=float(\"inf\")).transform(features).take (5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence = \"a b \" * 100 + \"a c \" * 10\n",
    "localDoc = [sentence, sentence]\n",
    "doc = sc.parallelize(localDoc).map(lambda line: line.split(\" \"))\n",
    "model = Word2Vec().setVectorSize(10).setSeed(42).fit(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.findSynonyms(\"a\",2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Word2Vec()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
